{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparazione ambiente"
      ],
      "metadata": {
        "id": "VZKs9lTfo_Rg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inizialmente ho ripetuto tutto ciò che ho già fatto e spiegato nel notebook con i samples completi, quindi:\n",
        "\n",
        "- Installato spark e parselmouth\n",
        "- Montato il drive\n",
        "- Aggiunto le funzioni utili\n",
        "- Aggiunto i percorsi delle cartelle"
      ],
      "metadata": {
        "id": "QMkHyEJ4pD4w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCvln2rJo4rP",
        "outputId": "c818c787-72da-43ee-b3ee-868268eef80b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-06 19:17:02--  https://downloads.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.208.237, 2a01:4f8:10a:39da::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400724056 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.5-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.5.5-bin-had 100%[===================>] 382.16M  28.1MB/s    in 14s     \n",
            "\n",
            "2025-03-06 19:17:16 (26.6 MB/s) - ‘spark-3.5.5-bin-hadoop3.tgz’ saved [400724056/400724056]\n",
            "\n",
            "<module 'pyspark.version' from '/content/spark-3.5.5-bin-hadoop3/python/pyspark/version.py'>\n"
          ]
        }
      ],
      "source": [
        "#Installazione spark\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://downloads.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.5-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.5-bin-hadoop3\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark\n",
        "print(pyspark.version)\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc=spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Installazione parselmouth\n",
        "\n",
        "!pip install praat-parselmouth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4HBHuQAphLY",
        "outputId": "60570ebe-d033-45ed-a8c3-64dc54f42f6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praat-parselmouth\n",
            "  Downloading praat_parselmouth-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from praat-parselmouth) (1.26.4)\n",
            "Downloading praat_parselmouth-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: praat-parselmouth\n",
            "Successfully installed praat-parselmouth-0.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Montaggio drive con audio e trascrizioni\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXnPAF5Ppja_",
        "outputId": "c8df1998-833d-4b2c-a8ce-4013eafcecd6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Funzioni utili\n",
        "\n",
        "import librosa\n",
        "import librosa.feature\n",
        "import numpy as np\n",
        "import parselmouth\n",
        "from parselmouth.praat import call\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Funzione per estrarre feature audio con librosa\n",
        "def extract_audio_features(audio_path, sr=22050, n_mfcc=13):\n",
        "    y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "    # Estrae MFCC\n",
        "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "    mfccs_mean = np.mean(mfccs, axis=1)  # Media per stabilizzare i dati\n",
        "\n",
        "    # Estrae RMSE (Energy)\n",
        "    rms = librosa.feature.rms(y=y)\n",
        "    rms_mean = np.mean(rms)\n",
        "\n",
        "    return np.concatenate((mfccs_mean, [rms_mean]))\n",
        "\n",
        "# Funzione per estrarre il pitch con Parselmouth\n",
        "def extract_pitch(audio_path):\n",
        "    snd = parselmouth.Sound(audio_path)\n",
        "    pitch = call(snd, \"To Pitch\", 0.0, 75, 600)\n",
        "    mean_pitch = call(pitch, \"Get mean\", 0, 0, \"Hertz\")  # Media del pitch\n",
        "\n",
        "    return np.array([mean_pitch])\n",
        "\n",
        "# Funzione per ottenere la rappresentazione testuale con TF-IDF\n",
        "def extract_text_features(text, vectorizer):\n",
        "    return vectorizer.transform([text]).toarray()[0]\n",
        "\n",
        "# Funzione per estrarre le trascrizioni dal file txt\n",
        "def load_transcriptions(txt_file):\n",
        "    transcriptions = {}\n",
        "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.read().strip().split(\"\\n\\n\")  # Divide il file in blocchi separati da una riga vuota\n",
        "\n",
        "    for block in lines:\n",
        "        lines = block.split(\"\\n\")  # Ogni blocco contiene nome file + trascrizione\n",
        "        if len(lines) >= 2:\n",
        "            filename = lines[0].strip()  # Prima riga: nome del file audio\n",
        "            transcript = \" \".join(lines[1:]).strip()  # Tutto il resto è la trascrizione\n",
        "            transcriptions[filename] = transcript  # Aggiungi al dizionario\n",
        "\n",
        "    return transcriptions"
      ],
      "metadata": {
        "id": "LxhLPKrupmPv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Percorsi delle cartelle\n",
        "audio_folder = \"/content/drive/MyDrive/audiozzi\"\n",
        "transcriptions_file = \"/content/drive/MyDrive/audiozzi/Trascrizioni.txt\""
      ],
      "metadata": {
        "id": "MYhS15CDp7gQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creazione samples"
      ],
      "metadata": {
        "id": "yCk3LKfNqhOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In seguito ho creato 2 set di samples:\n",
        "\n",
        "- 1 utilizzando solo Librosa e Parselmouth per estrarre le feature dagli audio.\n",
        "- 1 utilizzando solo TF-IDF per estrarre le feature dalle trascrizioni.\n",
        "\n",
        "I samples in entrambi i casi sono stati salvati sui file pkl per poter essere utilizzati nella fase successiva."
      ],
      "metadata": {
        "id": "-2IpJxp7qj7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creazione samples con solo Librosa e Parselmouth\n",
        "\n",
        "# Lista per contenere i dati finali\n",
        "datasetlp = []\n",
        "\n",
        "# Loop su tutti i file audio nella cartella\n",
        "for file in os.listdir(audio_folder):\n",
        "    if file.endswith(\".mp3\"):  # Filtra solo i file audio\n",
        "        audio_path = os.path.join(audio_folder, file)\n",
        "\n",
        "        # Determina l'etichetta (1 = Urgente, 0 = Normale) (sfrutto il fatto che ho rinominato i file)\n",
        "        label = 1 if file.endswith(\"u.mp3\") else 0\n",
        "\n",
        "        # Estrae feature audio\n",
        "        audio_features = extract_audio_features(audio_path)\n",
        "        pitch_feature = extract_pitch(audio_path)\n",
        "\n",
        "        # Concatena tutte le feature\n",
        "        sample = np.concatenate((audio_features, pitch_feature))\n",
        "\n",
        "        # Salva i dati\n",
        "        datasetlp.append({\n",
        "            \"filename\": file,\n",
        "            \"features\": sample,\n",
        "            \"label\": label  # Etichetta aggiunta\n",
        "        })\n",
        "\n",
        "# Converte in DataFrame per salvataggio\n",
        "dflp = pd.DataFrame(datasetlp)\n",
        "dflp.to_pickle(\"/content/drive/MyDrive/audiozzi/samplesSoloLibrosaParselmouth.pkl\")  # Salva in formato binario\n",
        "#df.to_csv(\"/content/drive/MyDrive/audiozzi/sss.csv\", index=False)\n",
        "\n",
        "print(f\"{len(dflp)} campioni salvati.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqAPbL-urDrI",
        "outputId": "c466047a-e578-4348-b01f-4c39da8da917"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300 campioni salvati.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creazione samples con solo TF-IDF\n",
        "\n",
        "# Carica le trascrizioni\n",
        "transcriptions = load_transcriptions(transcriptions_file)\n",
        "\n",
        "# Crea il vettorizzatore TF-IDF e lo adatta su tutte\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(list(transcriptions.values()))\n",
        "\n",
        "# Lista per contenere i dati finali\n",
        "datasettf = []\n",
        "\n",
        "# Loop su tutti i file audio nella cartella\n",
        "for file in os.listdir(audio_folder):\n",
        "    if file.endswith(\".mp3\"):  # Filtra solo i file audio\n",
        "        audio_path = os.path.join(audio_folder, file)\n",
        "\n",
        "        # Determina l'etichetta (1 = Urgente, 0 = Normale) (sfrutto il fatto che ho rinominato i file)\n",
        "        label = 1 if file.endswith(\"u.mp3\") else 0\n",
        "\n",
        "        # Recupera la trascrizione corrispondente\n",
        "        transcript = transcriptions.get(file, None)\n",
        "        if transcript is None:\n",
        "            print(f\"Nessuna trascrizione trovata per {file}, skippato.\")\n",
        "            continue\n",
        "\n",
        "        # Estrae feature testuali\n",
        "        sample = extract_text_features(transcript, vectorizer)\n",
        "\n",
        "        # Salva i dati\n",
        "        datasettf.append({\n",
        "            \"filename\": file,\n",
        "            \"features\": sample,\n",
        "            \"transcript\": transcript,\n",
        "            \"label\": label  # Etichetta aggiunta\n",
        "        })\n",
        "\n",
        "# Converte in DataFrame per salvataggio\n",
        "dftf = pd.DataFrame(datasettf)\n",
        "dftf.to_pickle(\"/content/drive/MyDrive/audiozzi/samplesSoloTFIDF.pkl\")  # Salva in formato binario\n",
        "#dftf.to_csv(\"/content/drive/MyDrive/audiozzi/ssssssss.csv\", index=False)\n",
        "\n",
        "print(f\"{len(dftf)} campioni salvati.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fiU9zzbrqvB",
        "outputId": "b467147a-39ca-491c-ca05-d8f78fcd7b0f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300 campioni salvati.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test modelli e confronto"
      ],
      "metadata": {
        "id": "7LPRELoctJ8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Infine ho recuperato i samples dai file binari e ci ho addestrato e testato i modelli per confrontare i risultati."
      ],
      "metadata": {
        "id": "-MuheaZftPBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solo Librosa e Parselmouth"
      ],
      "metadata": {
        "id": "KQfIL_8jtwPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.linalg import DenseVector\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Carica il dataset, pescando il file binario\n",
        "dflp = pd.read_pickle(\"/content/drive/MyDrive/audiozzi/samplesSoloLibrosaParselmouth.pkl\")\n",
        "\n",
        "# Converte in un DataFrame Spark con DenseVector\n",
        "spark_df = spark.createDataFrame([\n",
        "    Row(filename=row[\"filename\"],\n",
        "        features=DenseVector(row[\"features\"]),  # Converti in DenseVector\n",
        "        label=int(row[\"label\"]))  # Controllo label int\n",
        "    for _, row in dflp.iterrows()\n",
        "])\n",
        "\n",
        "#Divido training e test set in proporzione 80-20\n",
        "\n",
        "train_df_lp, test_df_lp = spark_df.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "xBlhZnWptac3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "\n",
        "# Definisce il modello di Random Forest\n",
        "rf_lp = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=50)\n",
        "\n",
        "# Addestra il modello sui dati di training\n",
        "model = rf_lp.fit(train_df_lp)\n",
        "\n",
        "# Genera le previsioni sul dataset di test\n",
        "predictions = model.transform(test_df_lp)\n",
        "\n",
        "#predictions.select(\"features\", \"label\", \"prediction\").show(10)\n",
        "\n",
        "# Definisce l'evaluator per l'accuratezza\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "# Calcola l'accuratezza\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuratezza del modello Random Forest: {accuracy * 100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "# Definisce il modello di regressione logistica\n",
        "lr_lp = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Addestra il modello\n",
        "lr_model = lr_lp.fit(train_df_lp)\n",
        "\n",
        "# Fa le previsioni\n",
        "lr_predictions = lr_model.transform(test_df_lp)\n",
        "\n",
        "# Valutazione\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "lr_accuracy = evaluator.evaluate(lr_predictions)\n",
        "print(f\"Accuratezza Logistic Regression: {lr_accuracy * 100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "# Definisci il modello Gradient Boosted Trees\n",
        "gbt_lp = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", maxIter=50)\n",
        "\n",
        "# Addestra il modello\n",
        "gbt_model = gbt_lp.fit(train_df_lp)\n",
        "\n",
        "# Fai le previsioni\n",
        "gbt_predictions = gbt_model.transform(test_df_lp)\n",
        "\n",
        "# Valutazione\n",
        "gbt_accuracy = evaluator.evaluate(gbt_predictions)\n",
        "print(f\"Accuratezza Gradient-Boosted Trees: {gbt_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8eMruaXt2w6",
        "outputId": "a3f3eb92-e234-4cd0-c050-860b6b8ef815"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuratezza del modello Random Forest: 94.23%\n",
            "Accuratezza Logistic Regression: 98.08%\n",
            "Accuratezza Gradient-Boosted Trees: 98.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solo TF-IDF"
      ],
      "metadata": {
        "id": "eEZJdFM4t6-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carica il dataset, pescando il file binario\n",
        "dftf = pd.read_pickle(\"/content/drive/MyDrive/audiozzi/samplesSoloTFIDF.pkl\")\n",
        "\n",
        "# Converte in un DataFrame Spark con DenseVector\n",
        "spark_df = spark.createDataFrame([\n",
        "    Row(filename=row[\"filename\"],\n",
        "        features=DenseVector(row[\"features\"]),  # Converti in DenseVector\n",
        "        label=int(row[\"label\"]))  # Controllo label int\n",
        "    for _, row in dftf.iterrows()\n",
        "])\n",
        "\n",
        "#Divido training e test set in proporzione 80-20\n",
        "\n",
        "train_df_tf, test_df_tf = spark_df.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "rRLB6pl6t8_Z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisce il modello di Random Forest\n",
        "rf_tf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=50)\n",
        "\n",
        "# Addestra il modello sui dati di training\n",
        "model = rf_tf.fit(train_df_tf)\n",
        "\n",
        "# Genera le previsioni sul dataset di test\n",
        "predictions = model.transform(test_df_tf)\n",
        "\n",
        "#predictions.select(\"features\", \"label\", \"prediction\").show(10)\n",
        "\n",
        "# Definisce l'evaluator per l'accuratezza\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "# Calcola l'accuratezza\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuratezza del modello Random Forest: {accuracy * 100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "# Definisce il modello di regressione logistica\n",
        "lr_tf = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Addestra il modello\n",
        "lr_model = lr_tf.fit(train_df_tf)\n",
        "\n",
        "# Fa le previsioni\n",
        "lr_predictions = lr_model.transform(test_df_tf)\n",
        "\n",
        "# Valutazione\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "lr_accuracy = evaluator.evaluate(lr_predictions)\n",
        "print(f\"Accuratezza Logistic Regression: {lr_accuracy * 100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "# Definisci il modello Gradient Boosted Trees\n",
        "gbt_tf = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", maxIter=50)\n",
        "\n",
        "# Addestra il modello\n",
        "gbt_model = gbt_tf.fit(train_df_tf)\n",
        "\n",
        "# Fai le previsioni\n",
        "gbt_predictions = gbt_model.transform(test_df_tf)\n",
        "\n",
        "# Valutazione\n",
        "gbt_accuracy = evaluator.evaluate(gbt_predictions)\n",
        "print(f\"Accuratezza Gradient-Boosted Trees: {gbt_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zlgxZ-4uILo",
        "outputId": "53804c7e-3dc3-4fbe-abb9-9e1519345b29"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuratezza del modello Random Forest: 100.00%\n",
            "Accuratezza Logistic Regression: 100.00%\n",
            "Accuratezza Gradient-Boosted Trees: 90.38%\n"
          ]
        }
      ]
    }
  ]
}