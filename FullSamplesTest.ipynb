{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installazione Spark e altro"
      ],
      "metadata": {
        "id": "bT4HU3PFxqcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per iniziare ho installato nell'ambiente Spark, Parselmouth e ho montato il drive su cui si trovano i file audio mp3 e il documento di testo con le trascrizioni."
      ],
      "metadata": {
        "id": "fnVtyPrSxu6T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH_8AU9OxN32",
        "outputId": "e3d6b2bc-5540-431a-ebc3-71307a293315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-06 14:25:12--  https://downloads.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 88.99.208.237, 135.181.214.104, 2a01:4f9:3a:2c57::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|88.99.208.237|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400724056 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.5-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.5.5-bin-had 100%[===================>] 382.16M  12.8MB/s    in 30s     \n",
            "\n",
            "2025-03-06 14:25:43 (12.6 MB/s) - ‘spark-3.5.5-bin-hadoop3.tgz’ saved [400724056/400724056]\n",
            "\n",
            "<module 'pyspark.version' from '/content/spark-3.5.5-bin-hadoop3/python/pyspark/version.py'>\n"
          ]
        }
      ],
      "source": [
        "#Installazione spark\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://downloads.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.5-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.5-bin-hadoop3\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark\n",
        "print(pyspark.version)\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc=spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Installazione parselmouth\n",
        "\n",
        "!pip install praat-parselmouth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG4H9xGIyDz1",
        "outputId": "169c0572-0c2e-4a67-e6f9-7923a0ee24f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praat-parselmouth\n",
            "  Downloading praat_parselmouth-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from praat-parselmouth) (1.26.4)\n",
            "Downloading praat_parselmouth-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: praat-parselmouth\n",
            "Successfully installed praat-parselmouth-0.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Montaggio drive con audio e trascrizioni\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoNHFtBDyD8x",
        "outputId": "a2b7426b-3545-438e-b8d1-a817702ba474"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funzioni utili"
      ],
      "metadata": {
        "id": "AZ-n7nujyYpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In seguito ho creato 3 funzioni per estrarre le feature che servono per comporre i samples e una funzione che estragga le trascrizioni dal file di testo e le inserisca in un dizionario."
      ],
      "metadata": {
        "id": "KJTEIXfIyd_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import librosa.feature\n",
        "import numpy as np\n",
        "import parselmouth\n",
        "from parselmouth.praat import call\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Funzione per estrarre feature audio con librosa\n",
        "def extract_audio_features(audio_path, sr=22050, n_mfcc=13):\n",
        "    y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "    # Estrae MFCC\n",
        "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "    mfccs_mean = np.mean(mfccs, axis=1)  # Media per stabilizzare i dati\n",
        "\n",
        "    # Estrae RMSE (Energy)\n",
        "    rms = librosa.feature.rms(y=y)\n",
        "    rms_mean = np.mean(rms)\n",
        "\n",
        "    return np.concatenate((mfccs_mean, [rms_mean]))\n",
        "\n",
        "# Funzione per estrarre il pitch con Parselmouth\n",
        "def extract_pitch(audio_path):\n",
        "    snd = parselmouth.Sound(audio_path)\n",
        "    pitch = call(snd, \"To Pitch\", 0.0, 75, 600)\n",
        "    mean_pitch = call(pitch, \"Get mean\", 0, 0, \"Hertz\")  # Media del pitch\n",
        "\n",
        "    return np.array([mean_pitch])\n",
        "\n",
        "# Funzione per ottenere la rappresentazione testuale con TF-IDF\n",
        "def extract_text_features(text, vectorizer):\n",
        "    return vectorizer.transform([text]).toarray()[0]\n",
        "\n",
        "# Funzione per estrarre le trascrizioni dal file txt\n",
        "def load_transcriptions(txt_file):\n",
        "    transcriptions = {}\n",
        "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.read().strip().split(\"\\n\\n\")  # Divide il file in blocchi separati da una riga vuota\n",
        "\n",
        "    for block in lines:\n",
        "        lines = block.split(\"\\n\")  # Ogni blocco contiene nome file + trascrizione\n",
        "        if len(lines) >= 2:\n",
        "            filename = lines[0].strip()  # Prima riga: nome del file audio\n",
        "            transcript = \" \".join(lines[1:]).strip()  # Tutto il resto è la trascrizione\n",
        "            transcriptions[filename] = transcript  # Aggiungi al dizionario\n",
        "\n",
        "    return transcriptions"
      ],
      "metadata": {
        "id": "wpT4zg4bytSc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creazione Samples completi (Librosa + Parselmouth + TF-IDF)"
      ],
      "metadata": {
        "id": "8WN6aCd2zUx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In questa fase ho creato i samples completi e li ho salvati in un file pkl sempre sul drive (il file si trova anche nella repository)."
      ],
      "metadata": {
        "id": "Scu9xCaDzbK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ricordo che la cartella con tutti gli audio e le trascrizioni si chiama \"audiozzi\"."
      ],
      "metadata": {
        "id": "9wObrlg-z63r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Percorsi delle cartelle\n",
        "audio_folder = \"/content/drive/MyDrive/audiozzi\"\n",
        "transcriptions_file = \"/content/drive/MyDrive/audiozzi/Trascrizioni.txt\"\n",
        "\n",
        "# Carica le trascrizioni su dizionario con la funzione creata prima\n",
        "transcriptions = load_transcriptions(transcriptions_file)\n",
        "\n",
        "# Crea il vettorizzatore TF-IDF e lo adatta su tutte\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(list(transcriptions.values()))\n",
        "\n",
        "# Lista per contenere i dati finali\n",
        "dataset = []\n",
        "\n",
        "# Loop su tutti i file audio nella cartella\n",
        "for file in os.listdir(audio_folder):\n",
        "    if file.endswith(\".mp3\"):  # Filtra solo i file audio\n",
        "        audio_path = os.path.join(audio_folder, file)\n",
        "\n",
        "        # Determina l'etichetta (1 = Urgente, 0 = Normale) (sfrutto il fatto che ho rinominato i file)\n",
        "        label = 1 if file.endswith(\"u.mp3\") else 0\n",
        "\n",
        "        # Recupera la trascrizione corrispondente\n",
        "        transcript = transcriptions.get(file, None)\n",
        "        if transcript is None:\n",
        "            print(f\"Nessuna trascrizione trovata per {file}, skippato.\")    # Mi aiuta se trova errori nel file di testo\n",
        "            continue\n",
        "\n",
        "        # Estrae feature audio\n",
        "        audio_features = extract_audio_features(audio_path)\n",
        "        pitch_feature = extract_pitch(audio_path)\n",
        "\n",
        "        # Estrae feature testuali\n",
        "        text_features = extract_text_features(transcript, vectorizer)\n",
        "\n",
        "        # Concatena tutte le feature\n",
        "        sample = np.concatenate((audio_features, pitch_feature, text_features))\n",
        "\n",
        "        # Salva i dati\n",
        "        dataset.append({\n",
        "            \"filename\": file,\n",
        "            \"features\": sample,\n",
        "            \"transcript\": transcript,\n",
        "            \"label\": label\n",
        "        })\n",
        "\n",
        "# Converte in DataFrame per salvataggio\n",
        "df = pd.DataFrame(dataset)\n",
        "df.to_pickle(\"/content/drive/MyDrive/audiozzi/samplesCompleti.pkl\")  # Salva in formato binario\n",
        "#df.to_csv(\"/content/drive/MyDrive/audiozzi/prepared_dataset.csv\", index=False)\n",
        "\n",
        "print(f\"{len(df)} campioni salvati.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tt-8EFwzz86",
        "outputId": "0a187019-57a6-4639-be0d-17c4c840f4db"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300 campioni salvati.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test dei modelli"
      ],
      "metadata": {
        "id": "iMUEDFu00p3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In questa fase ho recuperato i samples dal file binario (non è necessario se li abbiamo appena estratti nello stesso notebook).\n",
        "\n",
        "Ho diviso il dataset in training e test set (80-20).\n",
        "\n",
        "Ho applicato 3 modelli: Random Forest, Logistic Regression e Gradient Boosting Trees"
      ],
      "metadata": {
        "id": "05gmD4N00vc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.linalg import DenseVector\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Carica il dataset, pescando il file binario\n",
        "df = pd.read_pickle(\"/content/drive/MyDrive/audiozzi/samplesCompleti.pkl\")\n",
        "\n",
        "# Converte in un DataFrame Spark con DenseVector\n",
        "spark_df = spark.createDataFrame([\n",
        "    Row(filename=row[\"filename\"],\n",
        "        features=DenseVector(row[\"features\"]),  # Converti in DenseVector\n",
        "        label=int(row[\"label\"]))  # Controllo label int\n",
        "    for _, row in df.iterrows()\n",
        "])"
      ],
      "metadata": {
        "id": "e3EDOM1B1YFc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Divido training e test set in proporzione 80-20\n",
        "\n",
        "train_df, test_df = spark_df.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "ujP9IJvq1ju0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Primo modello: Random Forest\n",
        "\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "# Definisce il modello di Random Forest\n",
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=50)\n",
        "\n",
        "# Addestra il modello sui dati di training\n",
        "model = rf.fit(train_df)\n",
        "\n",
        "# Genera le previsioni sul dataset di test\n",
        "predictions = model.transform(test_df)\n",
        "\n",
        "#predictions.select(\"features\", \"label\", \"prediction\").show(10)\n",
        "\n",
        "# Definisce l'evaluator per l'accuratezza\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "# Calcola l'accuratezza\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuratezza del modello Random Forest: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A6d8MTH1qOb",
        "outputId": "eea1b505-67e2-4fb3-be1b-788379e3d20b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuratezza del modello Random Forest: 98.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Secondo modello: Logistic regression\n",
        "\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Definisce il modello di regressione logistica\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Addestra il modello\n",
        "lr_model = lr.fit(train_df)\n",
        "\n",
        "# Fa le previsioni\n",
        "lr_predictions = lr_model.transform(test_df)\n",
        "\n",
        "# Valutazione\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "lr_accuracy = evaluator.evaluate(lr_predictions)\n",
        "print(f\"Accuratezza Logistic Regression: {lr_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhZmsG3Z1vKE",
        "outputId": "b96a71e7-ab48-4322-cbe6-3e6cdb3a5c99"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuratezza Logistic Regression: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#terzo modello: gradient boosting trees\n",
        "\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "\n",
        "# Definisc il modello Gradient Boosted Trees\n",
        "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", maxIter=50)\n",
        "\n",
        "# Addestra il modello\n",
        "gbt_model = gbt.fit(train_df)\n",
        "\n",
        "# Fa le previsioni\n",
        "gbt_predictions = gbt_model.transform(test_df)\n",
        "\n",
        "# Valutazione\n",
        "gbt_accuracy = evaluator.evaluate(gbt_predictions)\n",
        "print(f\"Accuratezza Gradient-Boosted Trees: {gbt_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tER0N_zO1yAT",
        "outputId": "4f79649d-6353-465e-b800-c33d84a7512e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuratezza Gradient-Boosted Trees: 96.15%\n"
          ]
        }
      ]
    }
  ]
}